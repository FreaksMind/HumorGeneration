import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from openai import OpenAI

models_path = {
    "mistral": "E:\\HumorGeneration\\mistral_finetuned\\checkpoint-45",
    "qwen0.5b": "E:\\HumorGeneration\\qwen0.5b\\checkpoint-45",
    "qwen4b": "E:\\HumorGeneration\\qwen4b\\checkpoint-45"
}

device = "cuda" if torch.cuda.is_available() else "cpu"
MAX_NEW_TOKENS = 80

client = OpenAI(api_key="token")

def generate_joke(model_path, w1, w2):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None
    ).to(device)

    prompt = f"You are a helpful joke generator that creates funny jokes. Please generate a funny joke that contains the following words: \"{w1}\" and \"{w2}\". Please generate only the joke!"

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=0.8,
            top_p=0.95
        )

    text = tokenizer.decode(output[0], skip_special_tokens=True)
    joke = text.replace(prompt, "").strip()

    return joke


jokes = {}

for model_name, path in models_path.items():
    print(f"Generating joke with {model_name}...")
    jokes[model_name] = generate_joke(path)

print("\nGenerated Jokes:")
for model, joke in jokes.items():
    print(f"\n[{model}]\n{joke}")

voting_prompt = f"""
You are a professional humor judge.
Below are three jokes generated by different transformer models.
Vote for the BEST joke based on:
- Originality
- Humor
- Clarity

Jokes:
Model A (mistral):
{jokes['mistral']}

Model B (qwen0.5b):
{jokes['qwen0.5b']}

Model C (qwen4b):
{jokes['qwen4b']}

Respond in the following format:

Winner: <Model Name>
"""

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are an expert humor evaluator."},
        {"role": "user", "content": voting_prompt}
    ],
    temperature=0
)

print(response.choices[0].message.content)